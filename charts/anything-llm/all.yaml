---
# Source: anything-llm/charts/nvidia-device-plugin/templates/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: release-name
---
# Source: anything-llm/templates/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: release-name
---
# Source: anything-llm/charts/ollama/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ollama
  labels:
    helm.sh/chart: ollama-0.45.0
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.2.7"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: anything-llm/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: anything-llm-secret
  namespace: nvidia-device-plugin
  labels:
    helm.sh/chart: anything-llm-0.1.0
    app.kubernetes.io/name: anything-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  AUTH_TOKEN: cmVwbGFjZS1tZQ==
  JWT_SECRET: cmVwbGFjZS1tZQ==
---
# Source: anything-llm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: anything-llm-config
  namespace: release-name
  labels:
    helm.sh/chart: anything-llm-0.1.0
    app.kubernetes.io/name: anything-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
data:
  EMBEDDING_MODEL_MAX_CHUNK_LENGTH: "8192"
  EMBEDDING_MODEL_PREF: "nomic-embed-text:1.5"
  OLLAMA_MODEL_TOKEN_LIMIT: "8192"
  STORAGE_DIR: "/app/server/storage"
  TTS_PROVIDER: "native"
  VECTOR_DB: "lancedb"
  WHISPER_PROVIDER: "local"
---
# Source: anything-llm/charts/ollama/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    helm.sh/chart: ollama-0.45.0
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.2.7"
    app.kubernetes.io/managed-by: Helm
  name: ollama
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "50Gi"
---
# Source: anything-llm/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: server-storage
  namespace: release-name
  labels:
    helm.sh/chart: anything-llm-0.1.0
    app.kubernetes.io/name: anything-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Source: anything-llm/charts/ollama/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ollama
  labels:
    helm.sh/chart: ollama-0.45.0
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.2.7"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 11434
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: release-name
---
# Source: anything-llm/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: anything-llm
  namespace: release-name
  labels:
    helm.sh/chart: anything-llm-0.1.0
    app.kubernetes.io/name: anything-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: anything-llm
    app.kubernetes.io/instance: release-name
  ports:
    - protocol: TCP
      port: 3001
      targetPort: 3001
  type: ClusterIP
---
# Source: anything-llm/charts/nvidia-device-plugin/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset
  namespace: release-name
spec:
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        name: nvidia-device-plugin-ds
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: PUSTEKUCHEN
      tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
      priorityClassName: system-node-critical
      containers:
        - image: "nvcr.io/nvidia/k8s-device-plugin:v0.16.1"
          name: nvidia-device-plugin-ctr
          env:
            - name: FAIL_ON_INIT_ERROR
              value: "false"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          volumeMounts:
            - mountPath: /var/lib/kubelet/device-plugins
              name: device-plugin
      volumes:
        - hostPath:
            path: /var/lib/kubelet/device-plugins
          name: device-plugin
---
# Source: anything-llm/charts/ollama/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  labels:
    helm.sh/chart: ollama-0.45.0
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.2.7"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        helm.sh/chart: ollama-0.45.0
        app.kubernetes.io/name: ollama
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "0.2.7"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: ollama
      securityContext:
        {}
      containers:
        - name: ollama
          securityContext:
            {}
          image: "ollama/ollama:0.1.48"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 11434
              protocol: TCP
          env:
            - name: PATH
              value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
            - name: LD_LIBRARY_PATH
              value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: compute,utility
            - name: NVIDIA_VISIBLE_DEVICES
              value: all
          args:
          resources:
            limits:
              nvidia.com/gpu: 1
            requests: {}
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 6
          lifecycle:
            postStart:
              exec:
                command: [ "/bin/sh", "-c", "echo gemma2 | xargs -n1 /bin/ollama pull " ]
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - effect: NoSchedule
          key: ai
          operator: Equal
          value: "true"
---
# Source: anything-llm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: anything-llm
  namespace: release-name
  labels:
    helm.sh/chart: anything-llm-0.1.0
    app.kubernetes.io/name: anything-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: anything-llm
      app.kubernetes.io/instance: release-name
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: anything-llm
        app.kubernetes.io/instance: release-name
    spec:
      securityContext:
        fsGroup: 1000
      containers:
        - name: anything-llm
          image: "ghcr.io/la-cc/anything-llm:0.0.4"
          ports:
            - containerPort: 3001
          envFrom:
            - configMapRef:
                name: anything-llm-config
            - secretRef:
                name: anything-llm-secret
          volumeMounts:
            - name: server-storage
              mountPath: /app/server/storage
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 1000
            runAsGroup: 1000
            capabilities:
              add:
                - SYS_ADMIN
      volumes:
        - name: server-storage
          persistentVolumeClaim:
            claimName: server-storage
      restartPolicy: Always
---
# Source: anything-llm/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: anything-llm-ingress
  namespace: release-name
  labels:
    helm.sh/chart: anything-llm-0.1.0
    app.kubernetes.io/name: anything-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-dns"
    cert-manager.io/renew-before: "360h"
    nginx.ingress.kubernetes.io/rewrite-target: "/"
spec:
  ingressClassName: nginx
  rules:
    - host: llm.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: anything-llm
                port:
                  number: 3001
  tls:
    - hosts:
        - llm.example.com
      secretName: anything-llm-tls
---
# Source: anything-llm/charts/ollama/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "ollama-test-connection"
  labels:
    helm.sh/chart: ollama-0.45.0
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.2.7"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['ollama:11434']
  restartPolicy: Never
